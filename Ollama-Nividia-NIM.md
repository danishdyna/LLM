# Nvidia NIM
This tutorial explores Nvidia NIM - LLM Models in a container.
### Prerequisites:
* 
* 


## Ollama Client
Examples of running models in Ollama from Command Line:
```
ollama              -- Show usage
ollama list         -- List Models
ollama run phi3     -- Run Model
```
### Runtime Commands
```
  Ctrl+D, /bye    Exit
```
<sub><sub>
[#Mark-Down](https://daringfireball.net/projects/markdown/)
[#Ollama](https://github.com/ollama)
[#Ollama-Releases](https://github.com/ollama/ollama/releases)
