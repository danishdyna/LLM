# Ollama Run LLM
Examples of running LLM Models in Ollama.
## Ollama Client
Examples of running models in Ollama from Command Line:
```
ollama              -- Show usage
ollama list         -- List Models
ollama run phi3     -- Run Model
ollama ps           -- Running Models
```
Model commands
```
  /set            Set session variables
  /show           Show model information
  /load <model>   Load a session or model
  /save <model>   Save your current session
  /clear          Clear session context
  /bye            Exit
  /?, /help       Help for a command
  /? shortcuts    Help for keyboard shortcuts
```
## Ollama Server (Linux)
Ollama Server start and resource monitoring on Linux:
```
ollama serve -- Ollanma Server start (Linux)
nvidia-smi   -- Nvidia GPU resource usage (Linux)
```
