# Compute - Hardware Platform
This tutorial gives examples on how to run LLM Models on On-Prem Hardware Platforms.    
The $1K Windows PC gives easy general access to On-Prem LLMs with a limited resource frame.  
The $300 Linux GPU Cluster gives a state of the art powerful On-Prem access to LLMs. 
  
## Ollama for Windows
Download and install from
https://ollama.com/download  
Run Ollama from Command Console or PowerShell 
```
ollama list
ollama run phi
tasklist | findstr /i "ollama"
```
